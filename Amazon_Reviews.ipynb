{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Amazon_Reviews",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install pickle5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4xTqJdF4E5CF",
        "outputId": "2c2215e6-eb0c-42cc-c959-9361607138f5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.16.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.47)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.10.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.11.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: pickle5 in /usr/local/lib/python3.7/dist-packages (0.0.12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "1. order train_ds by length (sort of done)\n",
        "2. iteriamo sul dataloader facendo padding mano a mano (vedi codice vecchio notebook)\n",
        "'''\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "mAqQDLNiO8Tk",
        "outputId": "a84c1386-a5b3-4e45-9e79-91561129dc56"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n1. order train_ds by length (sort of done)\\n2. iteriamo sul dataloader facendo padding mano a mano (vedi codice vecchio notebook)\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "TloLi5EpBGcA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a56e7481-fd8f-4a53-f09d-c13bb64cfe8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Error loading corpus: Package 'corpus' not found in index\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package sentiwordnet to /root/nltk_data...\n",
            "[nltk_data]   Package sentiwordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import math\n",
        "import torch.nn as nn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Subset\n",
        "from torchtext import data\n",
        "import numpy as np\n",
        "import dataset\n",
        "import net\n",
        "import params\n",
        "from sklearn.metrics import accuracy_score\n",
        "'''\n",
        "def train_val_dataset(d, val_split=0.25):\n",
        "    train_idx, val_idx = train_test_split(list(d.data.index), test_size=val_split)\n",
        "    return Subset(d, train_idx), Subset(d, val_idx)\n",
        "'''\n",
        "def train_val_dataset(d, val_split=0.25):\n",
        "\n",
        "    train_idx, val_idx = train_test_split(list(d.data.index), test_size=val_split)\n",
        "   \n",
        "    # associa ad ogni elemento del dataset il numero di parole, e ordina in base al numero di parole\n",
        "    len_train = [(i, len(d.data[i].split(\" \"))) for i in train_idx]\n",
        "    len_train = sorted(len_train, key=lambda el: el[1], reverse = True)\n",
        "\n",
        "    #definisco indici ordinati da passare poi al dataloader\n",
        "    train_idx = [el[0] for el in len_train]\n",
        "\n",
        "    # stessa cosa per il test\n",
        "    len_val = [(i, len(d.data[i].split(\" \"))) for i in val_idx]\n",
        "    len_val = sorted(len_val, key=lambda el: el[1], reverse = True)\n",
        "\n",
        "    val_idx = [el[0] for el in len_val]\n",
        "    \n",
        "    return Subset(d, train_idx), Subset(d, val_idx)\n",
        "#'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "The colab session crashes after having createad the pickle file, so we report the items to remove in this list\n",
        "since the information gets lost when the session crashes.\n",
        "'''\n",
        "to_remove = [226, 371, 412, 1884, 1887, 3612, 4904, 5008, 5054, 5116, 5736, 5971, 6574, 8066, 8084, 8498, 8634, 8711, 9117, 9148, 9178, 9299, 9348, 9564, 9783, 9817, 10019, 10032, 10036, 10177]\n",
        "\n",
        "ds = dataset.AmazonDataset() \n",
        "ds.load_dataset()\n",
        "print(to_remove)\n",
        "ds.filter(to_remove)\n",
        "\n",
        "train_ds, test_ds = train_val_dataset(ds)\n",
        "\n",
        "# Define parameters\n",
        "batch_size = params.BATCH_SIZE\n",
        "hidden_dim = 128\n",
        "embedding_size = params.NUM_FEATURES  # lunghezza embedding (selezionato uno a caso perch√® tanto tutti hanno la stessa dimensione)\n",
        "dropout_rate = params.DROPOUT_RATE\n",
        "lr = params.LR\n",
        "epochs = params.NUM_EPOCHS\n",
        "best_loss = float('inf')"
      ],
      "metadata": {
        "id": "nn4FduDx3pG5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1be2a339-3f4d-47ea-db74-2b0ceab53666"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[226, 371, 412, 1884, 1887, 3612, 4904, 5008, 5054, 5116, 5736, 5971, 6574, 8066, 8084, 8498, 8634, 8711, 9117, 9148, 9178, 9299, 9348, 9564, 9783, 9817, 10019, 10032, 10036, 10177]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(ds.maximum_embedding_len)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ARXAMk6_qLyD",
        "outputId": "9e290427-1ab7-42db-b64d-94c7cc534d19"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def collate_batch(batch):\n",
        "  \n",
        "  label_list, text_list, = [], []\n",
        "  \n",
        "  for i, (text, label) in enumerate(batch):\n",
        "    label_list.append(label)\n",
        "    #print(len(text))\n",
        "    #print(type(text))\n",
        "    text = torch.stack((text)) # converts list of tensors to tensor of tensors\n",
        "    #text = torch.tensor(text, dtype=torch.float64)\n",
        "    #processed_text = torch.tensor(text) #, dtype=torch.int64\n",
        "    text_list.append(text)\n",
        "  \n",
        "  label_list = torch.tensor(label_list, dtype=torch.int64)\n",
        "  text_list = pad_sequence(text_list, batch_first=True, padding_value=0) #.size()\n",
        "  \n",
        "  return text_list, label_list\n"
      ],
      "metadata": {
        "id": "OJJHk_Hn2kBJ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch = [([torch.randn(3), torch.randn(3)], 2), ([torch.randn(3), torch.randn(3),torch.randn(3)],1)]\n",
        "text_list, label_list = collate_batch(batch)"
      ],
      "metadata": {
        "id": "k59HFztdKTLY"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# valuta di mettere un bucket iterator per averer ecnesioni di lunghezze simili\n",
        "train_loader = DataLoader(train_ds, batch_size=32, collate_fn=collate_batch, shuffle=True, num_workers=0)  # , collate_fn=None, pin_memory=False)\n",
        "\n",
        "# BUild Dataloader\n",
        "lstm_model = net.SentimentAnalysis(batch_size,\n",
        "                                   hidden_dim,\n",
        "                                   embedding_size,\n",
        "                                   dropout_rate)  # modificato LSTM con SentimentAnalysis (nome rete)\n",
        "\n",
        "# optimization algorithm\n",
        "optimizer = torch.optim.Adam(lstm_model.parameters(), lr=lr)\n",
        "\n",
        "#10036 --> 664 > 500\n",
        "#print(len(ds.data[500].split(\" \")))\n",
        "from torch.autograd import Variable\n",
        "to_train = True\n",
        "torch.autograd.set_detect_anomaly(True)\n",
        "ce = nn.CrossEntropyLoss()\n",
        "mse = nn.MSELoss()\n",
        "softmax = torch.nn.Softmax(dim = 1)\n",
        "# train and validate\n",
        "if to_train:\n",
        "    for epoch in range(epochs):\n",
        "        # training\n",
        "        epoch_loss = 0\n",
        "        epoch_acc = 0\n",
        "        for idxs, (batch, labels) in enumerate(train_loader):\n",
        "            \n",
        "            print(batch.size()) #32, 500, 3072 --> \n",
        "            \n",
        "            #batch = torch.stack(batch)\n",
        "            #batch = batch.permute(1, 0, 2)\n",
        "            \n",
        "            # optimizer.zero_grad()\n",
        "            predictions = lstm_model(batch)\n",
        "            \n",
        "            labels = torch.Tensor([x-1 for x in labels.data.numpy()]) #mapping classes 1-5 in 0-4\n",
        "            long_labels = labels.type(torch.LongTensor)\n",
        "            loss1 = 0.5 * ce(predictions, long_labels)\n",
        "            \n",
        "            #print(\"Float Preds\")\n",
        "            float_preds = torch.argmax(softmax(predictions), 1)\n",
        "            float_preds = float_preds.type(torch.FloatTensor)\n",
        "            \n",
        "            loss2 = 0.5 * mse(float_preds, labels)\n",
        "            loss = loss1.clone() + loss2.clone()\n",
        "            loss = Variable(loss, requires_grad = True)\n",
        "            accuracy = accuracy_score(float_preds.data, long_labels)\n",
        "            \n",
        "            # perform backpropagation\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            #print(\"ciao\")\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss = epoch_loss + loss  # .item()\n",
        "            epoch_acc = epoch_acc + accuracy\n",
        "            torch.cuda.empty_cache()\n",
        "        \n",
        "        train_loss, train_acc = epoch_loss / len(train_loader), epoch_acc / len(train_loader)\n",
        "\n",
        "        # save best model\n",
        "        if train_loss < best_loss:\n",
        "            best_valid_loss = train_loss\n",
        "            torch.save(lstm_model.state_dict(), 'saved_weights_BiLSTM.pt')\n",
        "\n",
        "        print(f'\\tEpoch; {epoch} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc * 100:.2f}%')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0hg5ytoiOseF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e1c5342-0767-4311-f410-7357593154b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 268, 3072])\n",
            "torch.Size([32, 150, 3072])\n",
            "torch.Size([32, 301, 3072])\n",
            "torch.Size([32, 245, 3072])\n",
            "torch.Size([32, 190, 3072])\n",
            "torch.Size([32, 257, 3072])\n",
            "torch.Size([32, 238, 3072])\n",
            "torch.Size([32, 218, 3072])\n",
            "torch.Size([32, 353, 3072])\n",
            "torch.Size([32, 201, 3072])\n",
            "torch.Size([32, 236, 3072])\n",
            "torch.Size([32, 223, 3072])\n",
            "torch.Size([32, 390, 3072])\n",
            "torch.Size([32, 226, 3072])\n",
            "torch.Size([32, 274, 3072])\n",
            "torch.Size([32, 248, 3072])\n",
            "torch.Size([32, 209, 3072])\n",
            "torch.Size([32, 461, 3072])\n",
            "torch.Size([32, 442, 3072])\n",
            "torch.Size([32, 116, 3072])\n",
            "torch.Size([32, 208, 3072])\n",
            "torch.Size([32, 385, 3072])\n",
            "torch.Size([32, 207, 3072])\n",
            "torch.Size([32, 182, 3072])\n",
            "torch.Size([32, 218, 3072])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load weights and make predictions\n",
        "lstm_model.load_state_dict(torch.load(\"saved_weights_BiLSTM.pt\"))\n",
        "epoch_loss = 0\n",
        "epoch_acc = 0\n",
        "\n",
        "lstm_model.eval()\n",
        "\n",
        "test_loader = DataLoader(test_ds, batch_size=32, shuffle=True, num_workers=0)  # , collate_fn=None, pin_memory=False)\n",
        "\n",
        "with torch.no_grad():\n",
        "    for idx, batch in test_loader:\n",
        "        text, text_lengths = batch.text\n",
        "\n",
        "        predictions = lstm_model(batch).squeeze(1)\n",
        "\n",
        "        loss = 0.5 * ce(predictions, batch.labels) + 0.5 * mse(predictions, batch.labels)\n",
        "\n",
        "        winners = predictions.argmax(dim=1)\n",
        "        corrects = (winners == batch.labels)\n",
        "        accuracy = corrects.sum().float() / float(batch.labels.size(0))\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += accuracy.item()\n",
        "\n",
        "test_loss, test_acc = epoch_loss / len(test_iterator), epoch_acc / len(test_iterator)\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc * 100:.2f}%')"
      ],
      "metadata": {
        "id": "1SCePsNiB4v7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}