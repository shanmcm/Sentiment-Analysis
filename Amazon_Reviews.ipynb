{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Amazon_Reviews.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shanmcm/Sentiment-Analysis-Amazon-Musical-Instruments-Reviews/blob/main/Amazon_Reviews.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pickle5\n",
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OnsaJPpR1Zwk",
        "outputId": "4cb63d58-b3c5-46a2-80af-ef31c097bc21"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pickle5 in /usr/local/lib/python3.7/dist-packages (0.0.12)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.16.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.47)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.10.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.11.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Subset\n",
        "import numpy as np\n",
        "import dataset\n",
        "import net\n",
        "import params\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "MLddvTBA4uPe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32fd284c-c79b-4a5f-b34e-1106ab14525e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Error loading corpus: Package 'corpus' not found in index\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package sentiwordnet to /root/nltk_data...\n",
            "[nltk_data]   Package sentiwordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "yLQaaBWxfTap"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_val_dataset(d, val_split=0.25):\n",
        "    train_idx, val_idx = train_test_split(list(d.data.index), test_size=val_split)\n",
        "    return Subset(d, train_idx), Subset(d, val_idx)"
      ],
      "metadata": {
        "id": "Dz--B9wPoCUL"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds = dataset.AmazonDataset()  # fare prova con anche dataset non caricato\n",
        "ds.load_dataset()\n",
        "ds.filter()\n",
        "ds.maximum_embedding_len = 500\n"
      ],
      "metadata": {
        "id": "oTr2Q-hTPbAp"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds, test_ds = train_val_dataset(ds)"
      ],
      "metadata": {
        "id": "oe66bTvcrEsB"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define parameters\n",
        "batch_size = params.BATCH_SIZE\n",
        "hidden_dim = 128\n",
        "embedding_size = params.NUM_FEATURES  # lunghezza embedding (selezionato uno a caso perchè tanto tutti hanno la stessa dimensione)\n",
        "dropout_rate = params.DROPOUT_RATE\n",
        "lr = params.LR\n",
        "epochs = params.NUM_EPOCHS\n",
        "best_loss = float('inf')\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True, num_workers=0)  # , collate_fn=None, pin_memory=False)"
      ],
      "metadata": {
        "id": "H28Hsmiktkmn"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the model\n",
        "lstm_model = net.SentimentAnalysis(batch_size,\n",
        "                                   hidden_dim,\n",
        "                                   embedding_size,\n",
        "                                   dropout_rate)  # modificato LSTM con SentimentAnalysis (nome rete)"
      ],
      "metadata": {
        "id": "Fx5k3y2jtncy"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# optimization algorithm\n",
        "optimizer = torch.optim.Adam(lstm_model.parameters(), lr=lr)"
      ],
      "metadata": {
        "id": "4gTyhcw8vDmU"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.autograd import Variable\n",
        "to_train = True\n",
        "torch.autograd.set_detect_anomaly(True)\n",
        "ce = nn.CrossEntropyLoss()\n",
        "mse = nn.MSELoss()\n",
        "softmax = torch.nn.Softmax(dim = 1)\n",
        "# train and validate\n",
        "if to_train:\n",
        "    for epoch in range(epochs):\n",
        "        # training\n",
        "        epoch_loss = 0\n",
        "        epoch_acc = 0\n",
        "        for idxs, (batch, labels) in enumerate(train_loader):\n",
        "            batch = torch.stack(batch)\n",
        "            batch = batch.permute(1, 0, 2)\n",
        "            \n",
        "            # optimizer.zero_grad()\n",
        "            predictions = lstm_model(batch)\n",
        "            \n",
        "            labels = torch.Tensor([x-1 for x in labels.data.numpy()]) #mapping classes 1-5 in 0-4\n",
        "            long_labels = labels.type(torch.LongTensor)\n",
        "            loss1 = 0.5 * ce(predictions, long_labels)\n",
        "            \n",
        "            #print(\"Float Preds\")\n",
        "            float_preds = torch.argmax(softmax(predictions), 1)\n",
        "            float_preds = float_preds.type(torch.FloatTensor)\n",
        "            \n",
        "            loss2 = 0.5 * mse(float_preds, labels)\n",
        "            loss = loss1.clone() + loss2.clone()\n",
        "            loss = Variable(loss, requires_grad = True)\n",
        "            accuracy = accuracy_score(float_preds.data, long_labels)\n",
        "            \n",
        "            # perform backpropagation\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            print(\"ciao\")\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss = epoch_loss + loss  # .item()\n",
        "            epoch_acc = epoch_acc + accuracy\n",
        "\n",
        "        train_loss, train_acc = epoch_loss / len(train_loader), epoch_acc / len(train_loader)\n",
        "\n",
        "        # save best model\n",
        "        if train_loss < best_loss:\n",
        "            best_valid_loss = train_loss\n",
        "            torch.save(lstm_model.state_dict(), 'saved_weights_BiLSTM.pt')\n",
        "\n",
        "        print(f'\\tEpoch; {epoch} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc * 100:.2f}%')"
      ],
      "metadata": {
        "id": "7w4Mx8dm3qpm",
        "outputId": "af46598b-cc2b-4884-d922-37235721724b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ciao\n",
            "ciao\n",
            "ciao\n",
            "ciao\n",
            "ciao\n",
            "ciao\n",
            "ciao\n",
            "ciao\n",
            "ciao\n",
            "ciao\n",
            "ciao\n",
            "ciao\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy"
      ],
      "metadata": {
        "id": "obz72cNgSN55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load weights and make predictions\n",
        "lstm_model.load_state_dict(torch.load(\"saved_weights_BiLSTM.pt\"))\n",
        "epoch_loss = 0\n",
        "epoch_acc = 0\n",
        "\n",
        "lstm_model.eval()\n",
        "\n",
        "test_loader = DataLoader(test_ds, batch_size=32, shuffle=True, num_workers=0)  # , collate_fn=None, pin_memory=False)\n",
        "\n",
        "with torch.no_grad():\n",
        "    for idx, batch in test_loader:\n",
        "        text, text_lengths = batch.text\n",
        "\n",
        "        predictions = lstm_model(batch).squeeze(1)\n",
        "\n",
        "        loss = 0.5 * ce(predictions, batch.labels) + 0.5 * mse(predictions, batch.labels)\n",
        "\n",
        "        winners = predictions.argmax(dim=1)\n",
        "        corrects = (winners == batch.labels)\n",
        "        accuracy = corrects.sum().float() / float(batch.labels.size(0))\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += accuracy.item()\n",
        "\n",
        "test_loss, test_acc = epoch_loss / len(test_iterator), epoch_acc / len(test_iterator)\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc * 100:.2f}%')"
      ],
      "metadata": {
        "id": "eSxRiQTornut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problemi:\n",
        "1. Stiamo passando alla rete le frasi di testo (dataset.data) non tensori. In che formato vanno passati al modello? getitem restituisce una lista di elementi, dove ogni elemento è un tensore (ogni tensore corrisponde ad una parola della recenzione e ha lunghezza 3072) e la lunghezza della recensione è variabile\n",
        "2. Attention weight va ancora chiamato\n",
        "3. In dataset.embedded_wordataset_dict ci sono parole (keys) con \"##\" davanti, credo non ci debbano essere? (vedi cella sotto)"
      ],
      "metadata": {
        "id": "tXJIPOiDJV1n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# x_train, x_test, y_train, y_test = train_test_split(dataset.embedded_data, dataset.labels, train_size=0.7, random_state=0, shuffle = False)\n",
        "\n",
        "\n",
        "# training_set = DatasetMapper(x_train, y_train)\n",
        "# test_set = DatasetMapper(x_test, y_test)\n",
        "\n",
        "# batch_size = params.BATCH_SIZE\n",
        "\n",
        "# loader_training = DataLoader(training_set, batch_size=batch_size)\n",
        "# loader_test = DataLoader(test_set)"
      ],
      "metadata": {
        "id": "w66eIo9O7tgE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "x_train, x_test, y_train, y_test = train_test_split(dataset.data, dataset.labels, train_size=0.7, random_state=0, shuffle = True)\n",
        "\n",
        "train_data = { 'features': x_train, 'labels': y_train}\n",
        "test_data = {'features': x_test, 'labels': y_test}\n",
        "\n",
        "train_data = pd.DataFrame(train_data)\n",
        "test_data = pd.DataFrame(test_data)\n",
        "'''"
      ],
      "metadata": {
        "id": "pZoAXraSUWan"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Define parameters\n",
        "# batch_size = params.BATCH_SIZE\n",
        "# hidden_dim = 128\n",
        "# embedding_size = len(dataset.__getitem__(0)[0]) #lunghezza embedding (selezionato uno a caso perchè tanto tutti hanno la stessa dimensione)\n",
        "# dropout_rate = params.DROPOUT_RATE\n",
        "# lr = params.LR\n",
        "# epochs = params.NUM_EPOCHS\n",
        "# best_loss = float('inf')\n",
        "\n",
        "# # BucketIterator : Defines an iterator that batches examples of similar lengths together to minimize the amount of padding needed.\n",
        "# # x_train, x_val, y_train, y_val = train_test_split(X_train, Y_train, train_size=0.8, random_state=0)\n",
        "# '''\n",
        "# train_iterator, test_iterator = data.BucketIterator.splits((train_data, test_data),\n",
        "#     batch_size = batch_size,\n",
        "#     sort_key = lambda x: len(x.text), # Sort the batches by text length size\n",
        "#     sort_within_batch = True,\n",
        "#     device = device)\n",
        "\n",
        "# train_iterator = DataLoader(train_data, batch_size=batch_size)\n",
        "# test_iterator = DataLoader(test_data, batch_size = batch_size)\n",
        "# '''\n",
        "# # Build the model\n",
        "# lstm_model = net.SentimentAnalysis(batch_size,\n",
        "#                                    hidden_dim,\n",
        "#                                    embedding_size,\n",
        "#                                    dropout_rate) #modificato LSTM con SentimentAnalysis (nome rete)\n",
        "\n",
        "# # optimization algorithm\n",
        "# optimizer = torch.optim.Adam(lstm_model.parameters(), lr=lr)"
      ],
      "metadata": {
        "id": "bRyMu10RVfrK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loader_training.dataset[9]"
      ],
      "metadata": {
        "id": "BCW2fE209oRS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to_train = True\n",
        "\n",
        "# # train and validate\n",
        "# if (to_train):\n",
        "\n",
        "#     for epoch in range(epochs):\n",
        "\n",
        "#         # training \n",
        "#         epoch_loss = 0\n",
        "#         epoch_acc = 0\n",
        "#         for batch in loader_training:\n",
        "#             optimizer.zero_grad()\n",
        "#             # retrieve text and no. of wordataset\n",
        "#             #text, text_lengths = batch.text\n",
        "            \n",
        "#             predictions = lstm_model(batch)    #(text, text_lengths) # batch_size, hidden_dim, vocab_size, window, dropout_rate\n",
        "#             loss = 0.5*nn.CrossEntropyLoss(predictions, batch.labels.squeeze())+ 0.5*nn.MSELoss(predictions, batch.labels.squeeze())\n",
        "#             '''\n",
        "#             winners = predictions.argmax(dim=1)\n",
        "#             corrects = (winners == batch.labels)\n",
        "#             accuracy = corrects.sum().float() / float(batch.labels.size(0))\n",
        "#             '''\n",
        "#             accuracy=accuracy_score(kept_labels, predictions)\n",
        "\n",
        "#             # perform backpropagation\n",
        "#             loss.backward()\n",
        "\n",
        "#             optimizer.step()\n",
        "\n",
        "#             epoch_loss += loss.item()\n",
        "#             epoch_acc += accuracy.item()\n",
        "            \n",
        "#         train_loss, train_acc = epoch_loss / len(train_iterator), epoch_acc / len(train_iterator)\n",
        "\n",
        "#         # save best model\n",
        "#         if train_loss < best_loss:\n",
        "#             best_valid_loss = train_loss\n",
        "#             torch.save(lstm_model.state_dict(), 'saved_weights_BiLSTM.pt')\n",
        "\n",
        "#         print(f'\\tEpoch; {epoch} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc * 100:.2f}%')  "
      ],
      "metadata": {
        "id": "piGVJCc1r3t7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # load weights and make predictions\n",
        "# lstm_model.load_state_dict(torch.load(\"saved_weights_BiLSTM.pt\"))\n",
        "# epoch_loss = 0\n",
        "# epoch_acc = 0\n",
        "\n",
        "# lstm_model.eval()\n",
        "\n",
        "# with torch.no_grad():\n",
        "#     for batch in test_iterator:\n",
        "#         text, text_lengths = batch.text\n",
        "\n",
        "#         predictions = lstm_model(batch).squeeze(1)\n",
        "\n",
        "#         loss = 0.5*nn.CrossEntropyLoss(predictions, batch.labels)+ 0.5*nn.MSELoss(predictions, batch.labels)\n",
        "\n",
        "#         winners = predictions.argmax(dim=1)\n",
        "#         corrects = (winners == batch.labels)\n",
        "#         accuracy = corrects.sum().float() / float(batch.labels.size(0))\n",
        "\n",
        "#         epoch_loss += loss.item()\n",
        "#         epoch_acc += accuracy.item()\n",
        "\n",
        "# test_loss, test_acc =  epoch_loss / len(test_iterator), epoch_acc / len(test_iterator)\n",
        "# print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc * 100:.2f}%')"
      ],
      "metadata": {
        "id": "fHlRh2BGW1sq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}