{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Amazon_Reviews.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shanmcm/Sentiment-Analysis-Amazon-Musical-Instruments-Reviews/blob/main/Amazon_Reviews.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import os\n",
        "import random\n",
        "import pandas as pd\n",
        "from torchtext import data\n",
        "import pickle5 as pickle"
      ],
      "metadata": {
        "id": "MLddvTBA4uPe"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_org = { 'a':'A', 'b':2, 'c':3.0 } \n",
        "print('DATA:', data_org)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WTNrUus5KMI9",
        "outputId": "81e5e9cc-fa58-467f-fd3d-49278226e926"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DATA: {'a': 'A', 'b': 2, 'c': 3.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_string = pickle.dumps(data_org)\n",
        "print('PICKLE:', data_string )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5cbXic77L8en",
        "outputId": "f8d5a104-dc6a-47ad-a58f-29fcf077b0bb"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PICKLE: b'\\x80\\x04\\x95 \\x00\\x00\\x00\\x00\\x00\\x00\\x00}\\x94(\\x8c\\x01a\\x94\\x8c\\x01A\\x94\\x8c\\x01b\\x94K\\x02\\x8c\\x01c\\x94G@\\x08\\x00\\x00\\x00\\x00\\x00\\x00u.'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('pickled_data_file.pkl', 'wb') as fid:\n",
        "  pickle.dump(data_org, fid)"
      ],
      "metadata": {
        "id": "VsMMpvjAK2dD"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('pickled_data_file.pkl', 'rb') as fid:\n",
        "  data3 = pickle.load(fid)"
      ],
      "metadata": {
        "id": "v7r_lFgbLMeh"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Data Before Write:', data_org)\n",
        "print('Data After  Read :', data3)\n",
        "print('EQUAL?:', (data_org == data3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fdp_-KGsLQNO",
        "outputId": "e27e69e5-ad52-4e8b-cb16-6055e945878a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data Before Write: {'a': 'A', 'b': 2, 'c': 3.0}\n",
            "Data After  Read : {'a': 'A', 'b': 2, 'c': 3.0}\n",
            "EQUAL?: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('amazonDataset.pkl', 'rb') as fid:\n",
        "  data3 = pickle.load(fid)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        },
        "id": "Z8AM1BtpMOZ-",
        "outputId": "520ce565-05df-4c73-bd12-695ccbb7adae"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-aabe37fdfe17>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'amazonDataset.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mdata3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: Can't get attribute 'AmazonDataset' on <module '__main__'>"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_string = pickle.dumps(data_org)\n",
        "print('PICKLE:', data_string )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p63NaqvcKOdG",
        "outputId": "0d758d9b-9ada-4c17-f73c-299138b8d9f7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PICKLE: b'\\x80\\x04\\x95 \\x00\\x00\\x00\\x00\\x00\\x00\\x00}\\x94(\\x8c\\x01a\\x94\\x8c\\x01A\\x94\\x8c\\x01b\\x94K\\x02\\x8c\\x01c\\x94G@\\x08\\x00\\x00\\x00\\x00\\x00\\x00u.'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('amazonDataset.pkl', 'rb') as fid:\n",
        "  data3 = pickle.load(fid)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        },
        "id": "E0FP3xyfGwte",
        "outputId": "a7a6bb04-729f-4b3c-c695-4499404182d6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-aabe37fdfe17>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'amazonDataset.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mdata3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: Can't get attribute 'AmazonDataset' on <module '__main__'>"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "1ioSAnVY577e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parte Lucia"
      ],
      "metadata": {
        "id": "S00AkynG2BE2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Qui c'Ã¨ un bozza sulla parte iniziale (settaggio parametri, creazione train test ecc)\n",
        "Faccio riferimento a una funzione cleanup_text che va riempita in base al preprocessing\n",
        "'''\n",
        "import params\n",
        "from lstm_cell import LSTMCell\n",
        "\n",
        "def cleanup_text(texts):\n",
        "  cleaned_text = []\n",
        "  for text in texts:\n",
        "      # remove punctuation, ecc....\n",
        "      text = re.sub('[!#?,.:\";]', ' ', text)\n",
        "     \n",
        "      cleaned_text.append(text)\n",
        "  return cleaned_text\n",
        "\n",
        "\n",
        "char_based = True\n",
        "if char_based:\n",
        "    tokenizer = lambda s: list(s) # char-based\n",
        "else:\n",
        "    tokenizer = lambda s: s.split() # word-based\n",
        "\n",
        "\n",
        "# hyper-parameters:\n",
        "lr = params.LR\n",
        "batch_size = params.BATCH_SIZE\n",
        "dropout_rate = params.DROPOUT_RATE\n",
        "#dropout_keep_prob = 0.5\n",
        "embedding_size = 300\n",
        "max_document_length = 100  # each sentence has until 100 words\n",
        "dev_size = 0.8 # split percentage to train\\validation data\n",
        "max_size = 5000 # maximum vocabulary size\n",
        "seed = params.SEED\n",
        "num_classes = params.NUM_CLASSES # output dimension of LSTM\n",
        "num_hidden_nodes = 93\n",
        "hidden_dim2 = 128\n",
        "num_layers = 2  # LSTM layers\n",
        "bi_directional = True\n",
        "epochs = params.NUM_EPOCHS\n",
        "\n",
        "Text = data.Field(preprocessing=cleanup_text, tokenize=tokenizer, batch_first=True, include_lengths=True, fix_length=max_document_length) # fix_length - make the sentences padded in the same lengths for all the batches\n",
        "Label = data.Field(sequential=False, use_vocab=False, pad_token=None, unk_token=None)\n",
        "\n",
        "# All files:\n",
        "fields = [('text', Text), ('labels', Label)]\n",
        "\n",
        "data_type = \"token\" # or: \"morph\"\n",
        "\n",
        "train_data, test_data = data.TabularDataset.splits(\n",
        " # https://pytorch.org/text/_modules/torchtext/data/dataset.html\n",
        ")\n",
        "\n",
        "train_data, valid_data = train_data.split(split_ratio=dev_size, random_state=random.seed(seed))\n",
        "\n",
        "# Build_vocab : It will first create a dictionary mapping all the unique words present in the train_data to an\n",
        "# index and then after it will use word embedding (random, Glove etc.) to map the index to the corresponding word embedding.\n",
        "Text.build_vocab(train_data, max_size=max_size)\n",
        "Label.build_vocab(train_data)\n",
        "vocab_size = len(Text.vocab)\n",
        "\n",
        "#  BucketIterator : Defines an iterator that batches examples of similar lengths together to minimize the amount of padding needed.\n",
        "# by setting sort_within_batch = True.\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits((train_data, valid_data, test_data),\n",
        "    batch_size = batch_size,\n",
        "    sort_key = lambda x: len(x.text), # Sort the batches by text length size\n",
        "    sort_within_batch = True,\n",
        "    device = device)"
      ],
      "metadata": {
        "id": "_cOpGKZ46ef3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parte Shannon"
      ],
      "metadata": {
        "id": "1i7BbSlC2DXo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "to_train = True\n",
        "pad_index = Text.vocab.stoi[Text.pad_token]\n",
        "\n",
        "# Build the model\n",
        "lstm_model = SentimentAnalysis(vocab_size, embedding_size, num_hidden_nodes, hidden_dim2 , num_classes, num_layers,\n",
        "                bi_directional, dropout_keep_prob, pad_index) #modificato LSTM con SentimentAnalysis (nome rete)\n",
        "\n",
        "# optimization algorithm\n",
        "optimizer = torch.optim.Adam(lstm_model.parameters(), lr=lr)\n",
        "\n",
        "# train and validate\n",
        "if (to_train):\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        # training \n",
        "        epoch_loss = 0\n",
        "        epoch_acc = 0\n",
        "        for batch in train_iterator:\n",
        "            optimizer.zero_grad()\n",
        "            # retrieve text and no. of words\n",
        "            text, text_lengths = batch.text\n",
        "\n",
        "            predictions = lstm_model(text, text_lengths) # batch_size, hidden_dim, vocab_size, window, dropout_rate\n",
        "            loss = 0.5*nn.CrossEntropyLoss(predictions, batch.labels.squeeze())+ 0.5*nn.MSELoss(predictions, batch.labels.squeeze())\n",
        "\n",
        "            winners = predictions.argmax(dim=1)\n",
        "            corrects = (winners == batch.labels)\n",
        "            accuracy = corrects.sum().float() / float(batch.labels.size(0))\n",
        "\n",
        "            # perform backpropagation\n",
        "            loss.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += accuracy.item()\n",
        "            \n",
        "        train_loss, train_acc = epoch_loss / len(train_iterator), epoch_acc / len(train_iterator)\n",
        "        \n",
        "        # validation\n",
        "        epoch_loss = 0\n",
        "        epoch_acc = 0\n",
        "\n",
        "        lstm_model.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in valid_iterator:\n",
        "                text, text_lengths = batch.text\n",
        "\n",
        "                predictions = lstm_model(text, text_lengths).squeeze(1)\n",
        "\n",
        "                loss = 0.5*nn.CrossEntropyLoss(predictions, batch.labels)+ 0.5*nn.MSELoss(predictions, batch.labels)\n",
        "\n",
        "                winners = predictions.argmax(dim=1)\n",
        "                corrects = (winners == batch.labels)\n",
        "                accuracy = corrects.sum().float() / float(batch.labels.size(0))\n",
        "\n",
        "                epoch_loss += loss.item()\n",
        "                epoch_acc += accuracy.item()\n",
        "\n",
        "        valid_loss, valid_acc =  epoch_loss / len(valid_iterator), epoch_acc / len(valid_iterator)\n",
        "\n",
        "\n",
        "        # save best model\n",
        "        if valid_loss < best_valid_loss:\n",
        "            best_valid_loss = valid_loss\n",
        "            torch.save(lstm_model.state_dict(), 'saved_weights_BiLSTM.pt')\n",
        "\n",
        "        print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc * 100:.2f}%')\n",
        "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc * 100:.2f}%')\n",
        "\n",
        "\n",
        "# load weights and make predictions\n",
        "lstm_model.load_state_dict(torch.load(\"saved_weights_BiLSTM.pt\"))\n",
        "epoch_loss = 0\n",
        "epoch_acc = 0\n",
        "\n",
        "lstm_model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_iterator:\n",
        "        text, text_lengths = batch.text\n",
        "\n",
        "        predictions = lstm_model(text, text_lengths).squeeze(1)\n",
        "\n",
        "        loss = 0.5*nn.CrossEntropyLoss(predictions, batch.labels)+ 0.5*nn.MSELoss(predictions, batch.labels)\n",
        "\n",
        "        winners = predictions.argmax(dim=1)\n",
        "        corrects = (winners == batch.labels)\n",
        "        accuracy = corrects.sum().float() / float(batch.labels.size(0))\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += accuracy.item()\n",
        "\n",
        "test_loss, test_acc =  epoch_loss / len(test_iterator), epoch_acc / len(test_iterator)\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc * 100:.2f}%')"
      ],
      "metadata": {
        "id": "n1bJRuB-2oKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git status"
      ],
      "metadata": {
        "id": "brRxcx6mquME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git add --all"
      ],
      "metadata": {
        "id": "hE8n0iTlq7eU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global user.email \"s289958@studenti.polito.it\" # mettere email del proprio github account"
      ],
      "metadata": {
        "id": "8-pypevKsOKm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git commit -m\"Added utils.py with useful functions\""
      ],
      "metadata": {
        "id": "mS_gTniVq_yW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git push"
      ],
      "metadata": {
        "id": "dTLGA2vOssJj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}