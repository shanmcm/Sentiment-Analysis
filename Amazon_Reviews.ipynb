{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Amazon_Reviews.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shanmcm/Sentiment-Analysis-Amazon-Musical-Instruments-Reviews/blob/main/Amazon_Reviews.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import os\n",
        "import random\n",
        "import pandas as pd\n",
        "from torchtext.legacy import data\n",
        "#import torchtext\n",
        "import pickle5 as pickle\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "\n",
        "import params\n",
        "import net"
      ],
      "metadata": {
        "id": "MLddvTBA4uPe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "234dffd9-6a0b-430e-d7a6-0973c2d11da7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Error loading corpus: Package 'corpus' not found in index\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package sentiwordnet to /root/nltk_data...\n",
            "[nltk_data]   Package sentiwordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "yLQaaBWxfTap"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "from dataset import AmazonDataset\n",
        "\n",
        "dataset = AmazonDataset()\n",
        "dataset.load_dataset()\n",
        "dataset.data\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive/')\n",
        "# DATA_PATH = \"/content/drive/MyDrive/NLP_Progetto\"\n",
        "# infile = open(DATA_PATH+'/amazonDataset.pkl','rb')\n",
        "# best_model2 = pickle.load(infile)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oTr2Q-hTPbAp",
        "outputId": "d881b784-7d3e-49bf-ca18-79bc4f9fb35a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.16.2)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.47)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.10.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.11.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "True\n",
            "weee\n",
            "ciao\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        much write exactly supposed filters pop sounds...\n",
              "1        product exactly quite affordablei realized dou...\n",
              "2        primary job device block breath would otherwis...\n",
              "3        nice windscreen protects mxl mic prevents pops...\n",
              "4        pop filter great looks performs like studio fi...\n",
              "                               ...                        \n",
              "10256                                 great expected thank\n",
              "10257    ive thinking trying nanoweb strings bit put hi...\n",
              "10258    tried coated strings past including elixirs ne...\n",
              "10259    well made elixir developed taylor guitars stri...\n",
              "10260    strings really quite good wouldnt call perfect...\n",
              "Name: reviewText, Length: 10254, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset.data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BjYlf4AbTKtQ",
        "outputId": "0077c4a1-768f-4976-cea0-4edfd1545126"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0        much write exactly supposed filters pop sounds...\n",
            "1        product exactly quite affordablei realized dou...\n",
            "2        primary job device block breath would otherwis...\n",
            "3        nice windscreen protects mxl mic prevents pops...\n",
            "4        pop filter great looks performs like studio fi...\n",
            "                               ...                        \n",
            "10256                                 great expected thank\n",
            "10257    ive thinking trying nanoweb strings bit put hi...\n",
            "10258    tried coated strings past including elixirs ne...\n",
            "10259    well made elixir developed taylor guitars stri...\n",
            "10260    strings really quite good wouldnt call perfect...\n",
            "Name: reviewText, Length: 10254, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.__getitem__(4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DIQl0teSTnJk",
        "outputId": "e5e1858c-0434-4996-d6a1-7e9e626c03f8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NN\n",
            "NN\n",
            "JJ\n",
            "NNS\n",
            "NNS\n",
            "IN\n",
            "NN\n",
            "NN\n",
            "PRP$\n",
            "NN\n",
            "VBG\n",
            "NNS\n",
            "NN\n",
            "NNS\n",
            "VBZ\n",
            "VBN\n",
            "VBG\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([0., 0., 0.,  ..., -0., -0., -0.]),\n",
              " tensor([0., 0., 0.,  ..., -0., -0., -0.]),\n",
              " tensor([0., 0., 0.,  ..., 0., -0., 0.]),\n",
              " tensor([0., 0., 0.,  ..., 0., -0., 0.]),\n",
              " tensor([-0., -0., 0.,  ..., -0., 0., -0.]),\n",
              " tensor([ 0.0350,  0.0094,  0.0722,  ..., -0.0513, -0.0029, -0.0458]),\n",
              " tensor([0., 0., 0.,  ..., -0., 0., -0.]),\n",
              " tensor([0., 0., 0.,  ..., -0., -0., -0.]),\n",
              " tensor([-0.7142,  0.4665,  1.1581,  ..., -0.9884,  0.5268, -1.0483]),\n",
              " tensor([-0.3330,  0.5854,  0.7720,  ..., -0.7944,  0.6782, -0.2575]),\n",
              " tensor([0., 0., 0.,  ..., 0., -0., -0.]),\n",
              " tensor([-0., 0., 0.,  ..., -0., 0., -0.]),\n",
              " tensor([-0., 0., 0.,  ..., -0., -0., -0.]),\n",
              " tensor([0., 0., 0.,  ..., -0., -0., 0.]),\n",
              " tensor([-0., -0., 0.,  ..., -0., 0., -0.]),\n",
              " tensor([-0., 0., 0.,  ..., -0., -0., -0.]),\n",
              " tensor([-0., -0., 0.,  ..., -0., -0., 0.])]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(dataset.data, dataset.labels, train_size=0.7, random_state=0)\n",
        "\n",
        "#train_data, valid_data = train_data.split(split_ratio=dev_size, random_state=random.seed(seed))\n",
        "train_data = { 'Features': x_train, 'Labels': y_train}\n",
        "test_data = {'Features': x_test, 'Labels': y_test}\n",
        "\n",
        "train_data = pd.DataFrame(train_data)\n",
        "test_data = pd.DataFrame(test_data)\n",
        "\n",
        "#  BucketIterator : Defines an iterator that batches examples of similar lengths together to minimize the amount of padding needed.\n",
        "# by setting sort_within_batch = True.\n",
        "train_iterator, test_iterator = data.BucketIterator.splits((train_data, test_data),\n",
        "    batch_size = params.BATCH_SIZE,\n",
        "    sort_key = lambda x: len(x.text), # Sort the batches by text length size\n",
        "    sort_within_batch = True,\n",
        "    device = device)"
      ],
      "metadata": {
        "id": "pZoAXraSUWan"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "to_train = True\n",
        "#pad_index = Text.vocab.stoi[Text.pad_token]\n",
        "batch_size = params.BATCH_SIZE\n",
        "hidden_dim = 128\n",
        "embedding_size = len(dataset.__getitem__(0)[0]) #lunghezza embedding (selezionato uno a caso perchè tanto tutti hanno la stessa dimensione)\n",
        "dropout_rate = params.DROPOUT_RATE\n",
        "# Build the model\n",
        "lstm_model = net.SentimentAnalysis(batch_size,\n",
        "                                   hidden_dim,\n",
        "                                   embedding_size,\n",
        "                                   dropout_rate) #modificato LSTM con SentimentAnalysis (nome rete)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRyMu10RVfrK",
        "outputId": "2f560414-5776-4e1c-d20b-5b0de7bb57d8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "JJ\n",
            "NN\n",
            "RB\n",
            "VBN\n",
            "NNS\n",
            "NN\n",
            "NNS\n",
            "NNS\n",
            "JJ\n",
            "NN\n",
            "CD\n",
            "JJS\n",
            "NNS\n",
            "NN\n",
            "NNS\n",
            "NN\n",
            "MD\n",
            "RB\n",
            "VB\n",
            "RB\n",
            "NN\n",
            "IN\n",
            "NN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lr = params.LR\n",
        "epochs = params.NUM_EPOCHS\n",
        "\n",
        "# optimization algorithm\n",
        "optimizer = torch.optim.Adam(lstm_model.parameters(), lr=lr)\n",
        "\n",
        "\n",
        "# train and validate\n",
        "if (to_train):\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        # training \n",
        "        epoch_loss = 0\n",
        "        epoch_acc = 0\n",
        "        for batch in train_iterator:\n",
        "            optimizer.zero_grad()\n",
        "            # retrieve text and no. of words\n",
        "            text, text_lengths = batch.text\n",
        "\n",
        "            predictions = lstm_model(text, text_lengths) # batch_size, hidden_dim, vocab_size, window, dropout_rate\n",
        "            loss = 0.5*nn.CrossEntropyLoss(predictions, batch.labels.squeeze())+ 0.5*nn.MSELoss(predictions, batch.labels.squeeze())\n",
        "\n",
        "            winners = predictions.argmax(dim=1)\n",
        "            corrects = (winners == batch.labels)\n",
        "            accuracy = corrects.sum().float() / float(batch.labels.size(0))\n",
        "\n",
        "            # perform backpropagation\n",
        "            loss.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += accuracy.item()\n",
        "            \n",
        "        train_loss, train_acc = epoch_loss / len(train_iterator), epoch_acc / len(train_iterator)\n",
        "        \n",
        "        # validation\n",
        "        epoch_loss = 0\n",
        "        epoch_acc = 0\n",
        "\n",
        "        lstm_model.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in valid_iterator:\n",
        "                text, text_lengths = batch.text\n",
        "\n",
        "                predictions = lstm_model(text, text_lengths).squeeze(1)\n",
        "\n",
        "                loss = 0.5*nn.CrossEntropyLoss(predictions, batch.labels)+ 0.5*nn.MSELoss(predictions, batch.labels)\n",
        "\n",
        "                winners = predictions.argmax(dim=1)\n",
        "                corrects = (winners == batch.labels)\n",
        "                accuracy = corrects.sum().float() / float(batch.labels.size(0))\n",
        "\n",
        "                epoch_loss += loss.item()\n",
        "                epoch_acc += accuracy.item()\n",
        "\n",
        "        valid_loss, valid_acc =  epoch_loss / len(valid_iterator), epoch_acc / len(valid_iterator)\n",
        "\n",
        "\n",
        "        # save best model\n",
        "        if valid_loss < best_valid_loss:\n",
        "            best_valid_loss = valid_loss\n",
        "            torch.save(lstm_model.state_dict(), 'saved_weights_BiLSTM.pt')\n",
        "\n",
        "        print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc * 100:.2f}%')\n",
        "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc * 100:.2f}%')\n",
        "\n",
        "\n",
        "# load weights and make predictions\n",
        "lstm_model.load_state_dict(torch.load(\"saved_weights_BiLSTM.pt\"))\n",
        "epoch_loss = 0\n",
        "epoch_acc = 0\n",
        "\n",
        "lstm_model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_iterator:\n",
        "        text, text_lengths = batch.text\n",
        "\n",
        "        predictions = lstm_model(text, text_lengths).squeeze(1)\n",
        "\n",
        "        loss = 0.5*nn.CrossEntropyLoss(predictions, batch.labels)+ 0.5*nn.MSELoss(predictions, batch.labels)\n",
        "\n",
        "        winners = predictions.argmax(dim=1)\n",
        "        corrects = (winners == batch.labels)\n",
        "        accuracy = corrects.sum().float() / float(batch.labels.size(0))\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += accuracy.item()\n",
        "\n",
        "test_loss, test_acc =  epoch_loss / len(test_iterator), epoch_acc / len(test_iterator)\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc * 100:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "piGVJCc1r3t7",
        "outputId": "7ac81bcc-2261-4d8d-b27d-6ff2efb99422"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3360\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 4800",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-96a9fbec1ab1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mepoch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mepoch_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0;31m# retrieve text and no. of words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchtext/legacy/data/iterator.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminibatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m                 \u001b[0;31m# fast-forward if loaded from state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchtext/legacy/data/iterator.py\u001b[0m in \u001b[0;36minit_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_random_state_this_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_shuffler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_restored_from_state\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchtext/legacy/data/iterator.py\u001b[0m in \u001b[0;36mcreate_batches\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    250\u001b[0m                                  self.batch_size_fn)\n\u001b[1;32m    251\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m             self.batches = pool(self.data(), self.batch_size,\n\u001b[0m\u001b[1;32m    253\u001b[0m                                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                                 \u001b[0mrandom_shuffler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_shuffler\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchtext/legacy/data/iterator.py\u001b[0m in \u001b[0;36mdata\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0mxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m             \u001b[0mxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_shuffler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0mxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchtext/legacy/data/iterator.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0mxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m             \u001b[0mxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_shuffler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0mxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3456\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3457\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3458\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3459\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3460\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3361\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3362\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3363\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3365\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhasnans\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 4800"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('amazonDataset.pkl', 'rb') as fid:\n",
        "  data3 = pickle.load(fid)"
      ],
      "metadata": {
        "id": "Z8AM1BtpMOZ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_string = pickle.dumps(data_org)\n",
        "print('PICKLE:', data_string )"
      ],
      "metadata": {
        "id": "p63NaqvcKOdG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('amazonDataset.pkl', 'rb') as fid:\n",
        "  data3 = pickle.load(fid)"
      ],
      "metadata": {
        "id": "E0FP3xyfGwte"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "1ioSAnVY577e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parte Lucia"
      ],
      "metadata": {
        "id": "S00AkynG2BE2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Qui c'è un bozza sulla parte iniziale (settaggio parametri, creazione train test ecc)\n",
        "Faccio riferimento a una funzione cleanup_text che va riempita in base al preprocessing\n",
        "'''\n",
        "import params\n",
        "from lstm_cell import LSTMCell\n",
        "\n",
        "def cleanup_text(texts):\n",
        "  cleaned_text = []\n",
        "  for text in texts:\n",
        "      # remove punctuation, ecc....\n",
        "      text = re.sub('[!#?,.:\";]', ' ', text)\n",
        "     \n",
        "      cleaned_text.append(text)\n",
        "  return cleaned_text\n",
        "\n",
        "\n",
        "char_based = True\n",
        "if char_based:\n",
        "    tokenizer = lambda s: list(s) # char-based\n",
        "else:\n",
        "    tokenizer = lambda s: s.split() # word-based\n",
        "\n",
        "\n",
        "# hyper-parameters:\n",
        "lr = params.LR\n",
        "batch_size = params.BATCH_SIZE\n",
        "dropout_rate = params.DROPOUT_RATE\n",
        "#dropout_keep_prob = 0.5\n",
        "embedding_size = 300\n",
        "max_document_length = 100  # each sentence has until 100 words\n",
        "dev_size = 0.8 # split percentage to train\\validation data\n",
        "max_size = 5000 # maximum vocabulary size\n",
        "seed = params.SEED\n",
        "num_classes = params.NUM_CLASSES # output dimension of LSTM\n",
        "num_hidden_nodes = 93\n",
        "hidden_dim2 = 128\n",
        "num_layers = 2  # LSTM layers\n",
        "bi_directional = True\n",
        "epochs = params.NUM_EPOCHS\n",
        "\n",
        "Text = data.Field(preprocessing=cleanup_text, tokenize=tokenizer, batch_first=True, include_lengths=True, fix_length=max_document_length) # fix_length - make the sentences padded in the same lengths for all the batches\n",
        "Label = data.Field(sequential=False, use_vocab=False, pad_token=None, unk_token=None)\n",
        "\n",
        "# All files:\n",
        "fields = [('text', Text), ('labels', Label)]\n",
        "\n",
        "data_type = \"token\" # or: \"morph\"\n",
        "\n",
        "train_data, test_data = data.TabularDataset.splits(\n",
        " # https://pytorch.org/text/_modules/torchtext/data/dataset.html\n",
        ")\n",
        "\n",
        "train_data, valid_data = train_data.split(split_ratio=dev_size, random_state=random.seed(seed))\n",
        "\n",
        "# Build_vocab : It will first create a dictionary mapping all the unique words present in the train_data to an\n",
        "# index and then after it will use word embedding (random, Glove etc.) to map the index to the corresponding word embedding.\n",
        "Text.build_vocab(train_data, max_size=max_size)\n",
        "Label.build_vocab(train_data)\n",
        "vocab_size = len(Text.vocab)\n",
        "\n",
        "#  BucketIterator : Defines an iterator that batches examples of similar lengths together to minimize the amount of padding needed.\n",
        "# by setting sort_within_batch = True.\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits((train_data, valid_data, test_data),\n",
        "    batch_size = batch_size,\n",
        "    sort_key = lambda x: len(x.text), # Sort the batches by text length size\n",
        "    sort_within_batch = True,\n",
        "    device = device)"
      ],
      "metadata": {
        "id": "_cOpGKZ46ef3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parte Shannon"
      ],
      "metadata": {
        "id": "1i7BbSlC2DXo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "to_train = True\n",
        "pad_index = Text.vocab.stoi[Text.pad_token]\n",
        "\n",
        "# Build the model\n",
        "lstm_model = SentimentAnalysis(vocab_size, embedding_size, num_hidden_nodes, hidden_dim2 , num_classes, num_layers,\n",
        "                bi_directional, dropout_keep_prob, pad_index) #modificato LSTM con SentimentAnalysis (nome rete)\n",
        "\n",
        "# optimization algorithm\n",
        "optimizer = torch.optim.Adam(lstm_model.parameters(), lr=lr)\n",
        "\n",
        "# train and validate\n",
        "if (to_train):\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        # training \n",
        "        epoch_loss = 0\n",
        "        epoch_acc = 0\n",
        "        for batch in train_iterator:\n",
        "            optimizer.zero_grad()\n",
        "            # retrieve text and no. of words\n",
        "            text, text_lengths = batch.text\n",
        "\n",
        "            predictions = lstm_model(text, text_lengths) # batch_size, hidden_dim, vocab_size, window, dropout_rate\n",
        "            loss = 0.5*nn.CrossEntropyLoss(predictions, batch.labels.squeeze())+ 0.5*nn.MSELoss(predictions, batch.labels.squeeze())\n",
        "\n",
        "            winners = predictions.argmax(dim=1)\n",
        "            corrects = (winners == batch.labels)\n",
        "            accuracy = corrects.sum().float() / float(batch.labels.size(0))\n",
        "\n",
        "            # perform backpropagation\n",
        "            loss.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += accuracy.item()\n",
        "            \n",
        "        train_loss, train_acc = epoch_loss / len(train_iterator), epoch_acc / len(train_iterator)\n",
        "        \n",
        "        # validation\n",
        "        epoch_loss = 0\n",
        "        epoch_acc = 0\n",
        "\n",
        "        lstm_model.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in valid_iterator:\n",
        "                text, text_lengths = batch.text\n",
        "\n",
        "                predictions = lstm_model(text, text_lengths).squeeze(1)\n",
        "\n",
        "                loss = 0.5*nn.CrossEntropyLoss(predictions, batch.labels)+ 0.5*nn.MSELoss(predictions, batch.labels)\n",
        "\n",
        "                winners = predictions.argmax(dim=1)\n",
        "                corrects = (winners == batch.labels)\n",
        "                accuracy = corrects.sum().float() / float(batch.labels.size(0))\n",
        "\n",
        "                epoch_loss += loss.item()\n",
        "                epoch_acc += accuracy.item()\n",
        "\n",
        "        valid_loss, valid_acc =  epoch_loss / len(valid_iterator), epoch_acc / len(valid_iterator)\n",
        "\n",
        "\n",
        "        # save best model\n",
        "        if valid_loss < best_valid_loss:\n",
        "            best_valid_loss = valid_loss\n",
        "            torch.save(lstm_model.state_dict(), 'saved_weights_BiLSTM.pt')\n",
        "\n",
        "        print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc * 100:.2f}%')\n",
        "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc * 100:.2f}%')\n",
        "\n",
        "\n",
        "# load weights and make predictions\n",
        "lstm_model.load_state_dict(torch.load(\"saved_weights_BiLSTM.pt\"))\n",
        "epoch_loss = 0\n",
        "epoch_acc = 0\n",
        "\n",
        "lstm_model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_iterator:\n",
        "        text, text_lengths = batch.text\n",
        "\n",
        "        predictions = lstm_model(text, text_lengths).squeeze(1)\n",
        "\n",
        "        loss = 0.5*nn.CrossEntropyLoss(predictions, batch.labels)+ 0.5*nn.MSELoss(predictions, batch.labels)\n",
        "\n",
        "        winners = predictions.argmax(dim=1)\n",
        "        corrects = (winners == batch.labels)\n",
        "        accuracy = corrects.sum().float() / float(batch.labels.size(0))\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += accuracy.item()\n",
        "\n",
        "test_loss, test_acc =  epoch_loss / len(test_iterator), epoch_acc / len(test_iterator)\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc * 100:.2f}%')"
      ],
      "metadata": {
        "id": "n1bJRuB-2oKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git status"
      ],
      "metadata": {
        "id": "brRxcx6mquME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git add --all"
      ],
      "metadata": {
        "id": "hE8n0iTlq7eU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global user.email \"s289958@studenti.polito.it\" # mettere email del proprio github account"
      ],
      "metadata": {
        "id": "8-pypevKsOKm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git commit -m\"Added utils.py with useful functions\""
      ],
      "metadata": {
        "id": "mS_gTniVq_yW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git push"
      ],
      "metadata": {
        "id": "dTLGA2vOssJj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}